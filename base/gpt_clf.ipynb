{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd99fb9c-d719-49e5-ba55-b02f4d2ab570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "def twenty_newsgroup_to_csv():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "\n",
    "    targets = pd.DataFrame( newsgroups_train.target_names, columns=['title'])\n",
    "\n",
    "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "    out.to_csv('./data/20_newsgroup.csv', index=False)\n",
    "    \n",
    "twenty_newsgroup_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f49a562-329f-47f9-87ef-74dcb7a688da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before null filtering: 11314\n",
      "Number of rows before token number filtering: 11096\n",
      "Number of rows data used: 11044\n"
     ]
    }
   ],
   "source": [
    "from openai.embeddings_utils import get_embeddings\n",
    "import openai, os, tiktoken, backoff\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\"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\"\"\"\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "batch_size = 2000\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "df = pd.read_csv('./data/20_newsgroup.csv')\n",
    "print(\"Number of rows before null filtering:\", len(df))\n",
    "df = df[df['text'].isnull() == False]\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "df[\"n_tokens\"] = df.text.apply(lambda x: len(encoding.encode(x)))\n",
    "print(\"Number of rows before token number filtering:\", len(df))\n",
    "df = df[df.n_tokens <= max_tokens]\n",
    "print(\"Number of rows data used:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448009d3-5fc5-460e-991f-52fea97f1186",
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x14615ef50 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/openai/embeddings_utils.py:35\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(list_of_text, engine)\u001b[0m\n\u001b[1;32m     33\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[0;32m---> 35\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     36\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(data, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# maintain the same order as input.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/openai/api_resources/embedding.py:34\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# This is only for the default case.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:115\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    114\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m--> 115\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/openai/api_requestor.py:181\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    172\u001b[0m     method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    173\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    180\u001b[0m )\n\u001b[0;32m--> 181\u001b[0m resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/openai/api_requestor.py:396\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 396\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/openai/api_requestor.py:429\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    430\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-text-embedding-ada-002 in organization org-icGIvCMKCHgKGTm25OqBBd4z on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m prompt_batches:\n\u001b[0;32m---> 14\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings_with_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_embeddings\n\u001b[1;32m     17\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mget_embeddings_with_backoff\u001b[0;34m(prompts, engine)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompts), batch_size):\n\u001b[1;32m      5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m prompts[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m----> 6\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gptl/lib/python3.10/site-packages/tenacity/__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n\u001b[1;32m    329\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(retry_state)\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x14615ef50 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def get_embeddings_with_backoff(prompts, engine):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        embeddings += get_embeddings(list_of_text=batch, engine=engine)\n",
    "    return embeddings\n",
    "\n",
    "prompts = df.text.tolist()\n",
    "prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n",
    "\n",
    "embeddings = []\n",
    "for batch in prompt_batches:\n",
    "    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n",
    "    embeddings += batch_embeddings\n",
    "\n",
    "df[\"embedding\"] = embeddings\n",
    "df.to_parquet(\"data/20_newsgroup_with_embedding.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d118dee-ce1f-4032-aa2f-9ec18ec5de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "embedding_df = pd.read_parquet(\"data/20_newsgroup_with_embedding.parquet\")\n",
    "\n",
    "matrix = np.vstack(embedding_df.embedding.values)\n",
    "num_of_clusters = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_of_clusters, init=\"k-means++\", n_init=10, random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "labels = kmeans.labels_\n",
    "embedding_df[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e4d5a5-5e75-4c26-be7a-b526cc3a3674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>count</th>\n",
       "      <th>rank1</th>\n",
       "      <th>rank1_count</th>\n",
       "      <th>rank2</th>\n",
       "      <th>rank2_count</th>\n",
       "      <th>first_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>522</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>432</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>6.0</td>\n",
       "      <td>82.76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>391</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>101</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>85.0</td>\n",
       "      <td>25.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1060</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>129</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>60.0</td>\n",
       "      <td>12.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>381</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>364</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>783</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>323</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>314.0</td>\n",
       "      <td>41.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>659</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>409</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>151.0</td>\n",
       "      <td>62.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>358</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>345</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>8</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>477</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>472</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>403</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>443</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>432</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>368</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>9.0</td>\n",
       "      <td>85.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>593</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>334</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>49.0</td>\n",
       "      <td>56.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>820</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>406</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>35.0</td>\n",
       "      <td>49.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>529</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>435</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>20.0</td>\n",
       "      <td>82.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>428</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>336</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>21.0</td>\n",
       "      <td>78.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>760</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>274</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>454</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>521</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>218</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>74.0</td>\n",
       "      <td>41.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>473</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.62%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  count                     rank1  rank1_count  \\\n",
       "0         0    522                 rec.autos          432   \n",
       "1         1    391  comp.sys.ibm.pc.hardware          101   \n",
       "2         2   1060        talk.politics.misc          129   \n",
       "3         3    381           rec.motorcycles          364   \n",
       "4         4    783  comp.sys.ibm.pc.hardware          323   \n",
       "5         5    659    soc.religion.christian          409   \n",
       "6         6    358                 sci.crypt          345   \n",
       "7         7     84   comp.os.ms-windows.misc            8   \n",
       "8         8    477          rec.sport.hockey          461   \n",
       "9         9    472                 sci.space          403   \n",
       "10       10    443            comp.windows.x          406   \n",
       "11       11    432     talk.politics.mideast          368   \n",
       "12       12    593   comp.os.ms-windows.misc          334   \n",
       "13       13    820        talk.politics.guns          406   \n",
       "14       14    529              misc.forsale          435   \n",
       "15       15    428           sci.electronics          336   \n",
       "16       16    760             comp.graphics          274   \n",
       "17       17    454                   sci.med          419   \n",
       "18       18    521               alt.atheism          218   \n",
       "19       19    473        rec.sport.baseball          457   \n",
       "\n",
       "                    rank2  rank2_count first_percentage  \n",
       "0   comp.sys.mac.hardware          6.0           82.76%  \n",
       "1   comp.sys.mac.hardware         85.0           25.83%  \n",
       "2      talk.religion.misc         60.0           12.17%  \n",
       "3   comp.sys.mac.hardware          1.0           95.54%  \n",
       "4   comp.sys.mac.hardware        314.0           41.25%  \n",
       "5      talk.religion.misc        151.0           62.06%  \n",
       "6   comp.sys.mac.hardware          1.0           96.37%  \n",
       "7   comp.sys.mac.hardware          8.0            9.52%  \n",
       "8                       0          0.0           96.65%  \n",
       "9   comp.sys.mac.hardware          1.0           85.38%  \n",
       "10                      0          0.0           91.65%  \n",
       "11     talk.religion.misc          9.0           85.19%  \n",
       "12  comp.sys.mac.hardware         49.0           56.32%  \n",
       "13     talk.religion.misc         35.0           49.51%  \n",
       "14  comp.sys.mac.hardware         20.0           82.23%  \n",
       "15  comp.sys.mac.hardware         21.0           78.50%  \n",
       "16  comp.sys.mac.hardware         34.0           36.05%  \n",
       "17                      0          0.0           92.29%  \n",
       "18     talk.religion.misc         74.0           41.84%  \n",
       "19                      0          0.0           96.62%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 统计每个cluster的数量\n",
    "new_df = embedding_df.groupby('cluster')['cluster'].count().reset_index(name='count')\n",
    "\n",
    "# 统计这个cluster里最多的分类的数量\n",
    "title_count = embedding_df.groupby(['cluster', 'title']).size().reset_index(name='title_count')\n",
    "first_titles = title_count.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))\n",
    "first_titles = first_titles.reset_index(drop=True)\n",
    "new_df = pd.merge(new_df, first_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')\n",
    "new_df = new_df.rename(columns={'title': 'rank1', 'title_count': 'rank1_count'})\n",
    "\n",
    "# 统计这个cluster里第二多的分类的数量\n",
    "second_titles = title_count[~title_count['title'].isin(first_titles['title'])]\n",
    "second_titles = second_titles.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))\n",
    "second_titles = second_titles.reset_index(drop=True)\n",
    "new_df = pd.merge(new_df, second_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')\n",
    "new_df = new_df.rename(columns={'title': 'rank2', 'title_count': 'rank2_count'})\n",
    "new_df['first_percentage'] = (new_df['rank1_count'] / new_df['count']).map(lambda x: '{:.2%}'.format(x))\n",
    "# 将缺失值替换为 0\n",
    "new_df.fillna(0, inplace=True)\n",
    "# 输出结果\n",
    "from IPython.display import display\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da293dd-8355-4a87-b275-10da6cc65ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0, Rank 1: rec.autos, Theme: 汽车硬件\n",
      "Cluster 1, Rank 1: comp.sys.ibm.pc.hardware, Theme: 电脑显示器\n",
      "Cluster 2, Rank 1: talk.politics.misc, Theme: 法律案例分析\n",
      "Cluster 3, Rank 1: rec.motorcycles, Theme: Cluster 3, Rank 1: rec.motorcycles, Theme: Cluster 3, Rank 1: rec.motorcycles, Theme: Cluster 3, Rank 1: rec.motorcycles, Theme: Cluster 3, Rank 1: rec.motorcycles, Theme: Cluster 3, Rank 1: rec.motorcycles, Theme: Cluster 3, Rank 1: rec.motorcycles, Theme: 骑行者与宠物\n",
      "Cluster 4, Rank 1: comp.sys.ibm.pc.hardware, Theme: 计算机硬件\n",
      "Cluster 5, Rank 1: soc.religion.christian, Theme: Cluster 5, Rank 1: soc.religion.christian, Theme: Cluster 5, Rank 1: soc.religion.christian, Theme: Cluster 5, Rank 1: soc.religion.christian, Theme: Cluster 5, Rank 1: soc.religion.christian, Theme: 宗教信仰与传统\n",
      "Cluster 6, Rank 1: sci.crypt, Theme: Cluster 6, Rank 1: sci.crypt, Theme: Cluster 6, Rank 1: sci.crypt, Theme: Cluster 6, Rank 1: sci.crypt, Theme: Cluster 6, Rank 1: sci.crypt, Theme: Cluster 6, Rank 1: sci.crypt, Theme: 秘密算法与安全\n",
      "Cluster 7, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 7, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 7, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 7, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 7, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 7, Rank 1: comp.os.ms-windows.misc, Theme: 科技发展\"\"\"\n",
      "Cluster 8, Rank 1: rec.sport.hockey, Theme: Cluster 8, Rank 1: rec.sport.hockey, Theme: Cluster 8, Rank 1: rec.sport.hockey, Theme: Cluster 8, Rank 1: rec.sport.hockey, Theme: 冰球赛事\n",
      "Cluster 9, Rank 1: sci.space, Theme: Cluster 9, Rank 1: sci.space, Theme: Cluster 9, Rank 1: sci.space, Theme: Cluster 9, Rank 1: sci.space, Theme: Cluster 9, Rank 1: sci.space, Theme: Cluster 9, Rank 1: sci.space, Theme: 太空探索\n",
      "Cluster 10, Rank 1: comp.windows.x, Theme: Cluster 10, Rank 1: comp.windows.x, Theme: Cluster 10, Rank 1: comp.windows.x, Theme: Cluster 10, Rank 1: comp.windows.x, Theme: Cluster 10, Rank 1: comp.windows.x, Theme: Cluster 10, Rank 1: comp.windows.x, Theme: Cluster 10, Rank 1: comp.windows.x, Theme: X Windows问题解决\n",
      "Cluster 11, Rank 1: talk.politics.mideast, Theme: 中东政治冲突\n",
      "Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: Cluster 12, Rank 1: comp.os.ms-windows.misc, Theme: 计算机硬件与软件\n",
      "Cluster 13, Rank 1: talk.politics.guns, Theme: 法律与政治讨论\n",
      "Cluster 14, Rank 1: misc.forsale, Theme: Cluster 14, Rank 1: misc.forsale, Theme: Cluster 14, Rank 1: misc.forsale, Theme: 音频设备和乐器\n",
      "Cluster 15, Rank 1: sci.electronics, Theme: Cluster 15, Rank 1: sci.electronics, Theme: Cluster 15, Rank 1: sci.electronics, Theme: Cluster 15, Rank 1: sci.electronics, Theme: Cluster 15, Rank 1: sci.electronics, Theme: Cluster 15, Rank 1: sci.electronics, Theme: 电子技术\n",
      "Cluster 16, Rank 1: comp.graphics, Theme: Cluster 16, Rank 1: comp.graphics, Theme: Cluster 16, Rank 1: comp.graphics, Theme: Cluster 16, Rank 1: comp.graphics, Theme: Cluster 16, Rank 1: comp.graphics, Theme: Cluster 16, Rank 1: comp.graphics, Theme: Cluster 16, Rank 1: comp.graphics, Theme: 计算机硬件与软件\n",
      "Cluster 17, Rank 1: sci.med, Theme: Cluster 17, Rank 1: sci.med, Theme: Cluster 17, Rank 1: sci.med, Theme: Cluster 17, Rank 1: sci.med, Theme: Cluster 17, Rank 1: sci.med, Theme: Cluster 17, Rank 1: sci.med, Theme: 营养和疾病\n",
      "Cluster 18, Rank 1: alt.atheism, Theme: Cluster 18, Rank 1: alt.atheism, Theme: Cluster 18, Rank 1: alt.atheism, Theme: Cluster 18, Rank 1: alt.atheism, Theme: Cluster 18, Rank 1: alt.atheism, Theme: Cluster 18, Rank 1: alt.atheism, Theme: Cluster 18, Rank 1: alt.atheism, Theme: 性取向与道德\n",
      "Cluster 19, Rank 1: rec.sport.baseball, Theme: Cluster 19, Rank 1: rec.sport.baseball, Theme: Cluster 19, Rank 1: rec.sport.baseball, Theme: Cluster 19, Rank 1: rec.sport.baseball, Theme: 棒球讨论\n"
     ]
    }
   ],
   "source": [
    "items_per_cluster = 10\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def f(i):\n",
    "    cluster_name = new_df[new_df.cluster == i].iloc[0].rank1\n",
    "    print(f\"Cluster {i}, Rank 1: {cluster_name}, Theme:\", end=\" \")\n",
    "\n",
    "    content = \"\\n\".join(\n",
    "        embedding_df[embedding_df.cluster == i].text.sample(items_per_cluster, random_state=42).values\n",
    "    )\n",
    "    response = openai.Completion.create(\n",
    "        model=COMPLETIONS_MODEL,\n",
    "        prompt=f'''我们想要给下面的内容，分组成有意义的类别，以便我们可以对其进行总结。请根据下面这些内容的共同点，总结一个50个字以内的新闻组的名称。比如 “PC硬件”\\n\\n内容:\\n\"\"\"\\n{content}\\n\"\"\"新闻组名称：''',\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "    )\n",
    "    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n",
    "\n",
    "for i in range(num_of_clusters):\n",
    "    f(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038d899-8b33-40cd-89b4-3fc8fd6a6cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
